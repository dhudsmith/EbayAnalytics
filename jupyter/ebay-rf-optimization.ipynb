{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-05-03T23:12:53.901368",
     "start_time": "2016-05-03T23:12:53.700087"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pylab as pylab\n",
    "pylab.rcParams['figure.figsize'] = (10, 6)\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import OrderedDict\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "import pandas as pd\n",
    "\n",
    "# Import the binary classification dataset\n",
    "data = pd.read_csv('../Data/ebay_data_rf.csv', index_col=False)\n",
    "data.drop('value', axis=1, inplace=True)\n",
    "\n",
    "# Separate the target and inputs\n",
    "y = data.sellingState\n",
    "X = data.drop('sellingState', axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dependence of OOB error on n_estimators\n",
    "\n",
    "The \"out-of-bag\" (OOB) error can be calculated while training a random forest, and gives a reasonable measure of the model validity. \n",
    "\n",
    "The 'n_estimators' parameter specifies the number of trees in the random forest. In general, larger numbers of trees improve the accuracy of the model, but increasing 'n_estimators' gives diminishing returns at a certain point. Large 'n_estimators' increases the training time and prediction time of the model, which we would like to avoid since ultimately our model will be exposed via an interactive web app."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate the OOB error rates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-05-03T23:13:36.100271",
     "start_time": "2016-05-03T23:12:53.903456"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-b748ea10f283>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     46\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmin_estimators\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_estimators\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m         \u001b[0mclf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_params\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_estimators\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 48\u001b[0;31m         \u001b[0mclf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     49\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m         \u001b[0;31m# Record the OOB error for each `n_estimators=i` setting.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/dhudsonsmith/anaconda3/lib/python3.5/site-packages/sklearn/ensemble/forest.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m    294\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    295\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moob_score\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 296\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_set_oob_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    297\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    298\u001b[0m         \u001b[0;31m# Decapsulate classes_ attributes\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/dhudsonsmith/anaconda3/lib/python3.5/site-packages/sklearn/ensemble/forest.py\u001b[0m in \u001b[0;36m_set_oob_score\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m    391\u001b[0m                 estimator.random_state, n_samples)\n\u001b[1;32m    392\u001b[0m             p_estimator = estimator.predict_proba(X[unsampled_indices, :],\n\u001b[0;32m--> 393\u001b[0;31m                                                   check_input=False)\n\u001b[0m\u001b[1;32m    394\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    395\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_outputs_\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/dhudsonsmith/anaconda3/lib/python3.5/site-packages/sklearn/tree/tree.py\u001b[0m in \u001b[0;36mpredict_proba\u001b[0;34m(self, X, check_input)\u001b[0m\n\u001b[1;32m    671\u001b[0m         \"\"\"\n\u001b[1;32m    672\u001b[0m         \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_validate_X_predict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcheck_input\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 673\u001b[0;31m         \u001b[0mproba\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtree_\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    674\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    675\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_outputs_\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# NOTE: Setting the `warm_start` construction parameter to `True` disables\n",
    "# support for paralellised ensembles but is necessary for tracking the OOB\n",
    "# error trajectory during training.\n",
    "RANDOM_STATE = 123\n",
    "ensemble_clfs = [\n",
    "    (\"RandomForestClassifier, max_features=5\",\n",
    "        RandomForestClassifier(warm_start=True,\n",
    "                               max_features=\"sqrt\",\n",
    "                               oob_score=True,\n",
    "                               random_state=RANDOM_STATE)),\n",
    "    (\"RandomForestClassifier, max_features=7\",\n",
    "        RandomForestClassifier(warm_start=True,\n",
    "                               max_features=7,\n",
    "                               oob_score=True,\n",
    "                               random_state=RANDOM_STATE)),\n",
    "    (\"RandomForestClassifier, max_features=21\",\n",
    "        RandomForestClassifier(warm_start=True,\n",
    "                               max_features=None,\n",
    "                               oob_score=True,\n",
    "                               random_state=RANDOM_STATE))\n",
    "]\n",
    "\n",
    "# ensemble_clfs = [\n",
    "#     (\"GradientBoostingClassifier, max_features=5\",\n",
    "#         GradientBoostingClassifier(warm_start=True,\n",
    "#                                max_features=5,\n",
    "#                                oob_score=True)),\n",
    "#     (\"GradientBoostingClassifier, max_features=7\",\n",
    "#         GradientBoostingClassifier(warm_start=True,\n",
    "#                                max_features=7,\n",
    "#                                oob_score=True)),\n",
    "#     (\"GradientBoostingClassifier, max_features=All\",\n",
    "#         GradientBoostingClassifier(warm_start=True,\n",
    "#                                max_features=None,\n",
    "#                                oob_score=True))\n",
    "# ]\n",
    "\n",
    "# Map a classifier name to a list of (<n_estimators>, <error rate>) pairs.\n",
    "error_rate = OrderedDict((label, []) for label, _ in ensemble_clfs)\n",
    "\n",
    "# Range of `n_estimators` values to explore.\n",
    "min_estimators = 40\n",
    "max_estimators = 140\n",
    "\n",
    "for label, clf in ensemble_clfs:\n",
    "    for i in range(min_estimators, max_estimators + 1):\n",
    "        clf.set_params(n_estimators=i)\n",
    "        clf.fit(X, y)\n",
    "\n",
    "        # Record the OOB error for each `n_estimators=i` setting.\n",
    "        oob_error = 1 - clf.oob_score_\n",
    "        error_rate[label].append((i, oob_error))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate the \"OOB error rate\" vs. \"n_estimators\" plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-05-03T23:13:36.100902",
     "start_time": "2016-05-04T03:12:53.703Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for label, clf_err in error_rate.items():\n",
    "    xs, ys = zip(*clf_err)\n",
    "    plt.plot(xs, ys, label=label)\n",
    "\n",
    "plt.title(\"Dependence of OOB error on number of trees (n_estimators)\")\n",
    "plt.xlim(min_estimators, max_estimators)\n",
    "plt.xlabel(\"n_estimators\")\n",
    "plt.ylabel(\"OOB error rate\")\n",
    "plt.legend(loc=\"upper right\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The 'max_features' parameter specifies the number of features to consider at each split in a decision tree in the random forest. Apparently, the data has more than 5 predictive features. For 'max_features' = 7, the OOB error rate settels down by about 'n_estimators' = 80."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dependence of OOB errors on \"max_features\"\n",
    "\n",
    "We now determine the optimum value of 'max_features' for 'n_estimators' = 80."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate the OOB error rates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-05-03T23:13:36.101387",
     "start_time": "2016-05-04T03:12:53.706Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ensemble_clfs = [\n",
    "    (\"RandomForestClassifier, max_features='Variable'\",\n",
    "        RandomForestClassifier(n_estimators = 200,\n",
    "                               oob_score=True,\n",
    "                               random_state=RANDOM_STATE)),\n",
    "]\n",
    "\n",
    "# Map a classifier name to a list of (<n_estimators>, <error rate>) pairs.\n",
    "error_rate = OrderedDict((label, []) for label, _ in ensemble_clfs)\n",
    "\n",
    "# Range of `n_estimators` values to explore.\n",
    "min_par = 1\n",
    "max_par = 20\n",
    "\n",
    "for label, clf in ensemble_clfs:\n",
    "    for i in range(min_par, max_par + 1,1):\n",
    "        clf.set_params(max_features=i)\n",
    "        clf.fit(X, y)\n",
    "\n",
    "        # Record the OOB error for each `max_estimators=i` setting.\n",
    "        oob_error = 1 - clf.oob_score_\n",
    "        error_rate[label].append((i, oob_error))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate the \"OOB error rate\" vs. \"n_estimators\" plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-05-03T23:13:36.101955",
     "start_time": "2016-05-04T03:12:53.709Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Generate the \"OOB error rate\" vs. \"n_estimators\" plot.\n",
    "for label, clf_err in error_rate.items():\n",
    "    xs, ys = zip(*clf_err)\n",
    "    plt.plot(xs, ys, label=label)\n",
    "\n",
    "plt.title(\"Depence of OOB error on 'max_features'\")\n",
    "plt.xlim(min_par, max_par)\n",
    "plt.xlabel(\"max_features\")\n",
    "plt.ylabel(\"OOB error rate\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The plot shows the OOB error rate as a function of 'max_features' for max features from 1 to 21. 21 is the total number of features in our data set. The obvious candidates based on the plot are 'max_features' = 7 and 'max_features' = 10. Out of a preference for simpler models, we will use 'max_features' = 7."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summary\n",
    "We considered the dependence of the OOB error rate for a random forest classification model on the random forest model parameters 'n_estimators' and 'max_features'. We found that that 'n_estimators' = 80 and 'max_features' = 7 was a reasonable choice of model parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  },
  "toc": {
   "toc_cell": false,
   "toc_number_sections": true,
   "toc_threshold": 6,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
